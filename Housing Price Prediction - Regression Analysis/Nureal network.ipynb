{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "import statsmodels.api as sm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"train.csv\")\n",
    "#df1.head()\n",
    "df2 = pd.read_csv('test.csv')\n",
    "#df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and drop duplicate columns\n",
    "duplicated_columns = df1.columns[df1.columns.duplicated()]\n",
    "df_no_duplicates = df1.drop(columns=duplicated_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and drop duplicate columns\n",
    "duplicated_columns = df2.columns[df2.columns.duplicated()]\n",
    "df_no_duplicates = df2.drop(columns=duplicated_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop(columns=['sub_area'])\n",
    "df2 = df2.drop(columns=['sub_area','row ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onehot = pd.get_dummies(df1)\n",
    "df_onehot.dtypes\n",
    "test_onehot = pd.get_dummies(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onehot = pd.get_dummies(df1,drop_first=True)\n",
    "test_onehot = pd.get_dummies(df2,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = df1.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Label encode categorical columns\n",
    "label_encoder = LabelEncoder()\n",
    "for col in categorical_columns:\n",
    "   df1[col] = label_encoder.fit_transform(df1[col])\n",
    "   df2[col] = label_encoder.transform(df2[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_onehot.drop(columns=['price_doc',],axis=1)\n",
    "y = df_onehot['price_doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1.drop(columns=['price_doc',],axis=1)\n",
    "y = df1['price_doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "df2_encoded = imputer.fit_transform(test_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "df2_standardized = scaler.fit_transform(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_testscaled = scaler.fit_transform(test_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "df2_encoded_scaled = scaler.fit_transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "variance_threshold = 0.001 # Set your desired threshold\n",
    "selector = VarianceThreshold(threshold=variance_threshold)\n",
    "X_train_high_variance = selector.fit_transform(X_standardized)\n",
    "X_test_high_variance = selector.transform(df2_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_high_variance.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_high_variance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy y in y_test\n",
    "y_test = y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covert x_scaled to dataframe\n",
    "X_scaled = pd.DataFrame(X_scaled)\n",
    "#convert x_testscaled to dataframe\n",
    "X_testscaled = pd.DataFrame(df2_encoded_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covert x_scaled to dataframe\n",
    "X_scaled = pd.DataFrame(X_standardized)\n",
    "#convert x_testscaled to dataframe\n",
    "X_testscaled = pd.DataFrame(df2_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:\n",
      "Index([  0,   1,   2,   3,   5,   6,   8,   9,  10,  11,\n",
      "       ...\n",
      "       248, 250, 251, 252, 253, 256, 259, 265, 266, 269],\n",
      "      dtype='object', length=174)\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "# Convert scaled_df1 to a DataFrame without specifying columns\n",
    "\n",
    "# Add a constant term to the feature matrix\n",
    "X_with_const = sm.add_constant(X_scaled)\n",
    "\n",
    "# Fit a linear regression model\n",
    "model = sm.OLS(y, X_with_const).fit()\n",
    "\n",
    "# Get p-values for each feature\n",
    "p_values = model.pvalues[1:]  # Exclude the constant term\n",
    "\n",
    "# Set your desired threshold for p-value\n",
    "threshold = 0.05\n",
    "\n",
    "# Filter features based on p-value\n",
    "selected_features = p_values[p_values < threshold].index\n",
    "\n",
    "# Display selected features\n",
    "print(\"Selected Features:\")\n",
    "print(selected_features)\n",
    "\n",
    "# Select columns in the DataFrame\n",
    "X_selected = X_scaled[selected_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep those columns in X_testscaled that are in X_selected\n",
    "X_testscaled = X_testscaled[X_selected.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181507, 174)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_selected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77789, 174)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_testscaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kBest = SelectKBest(k=20, score_func=f_regression)\n",
    "X_train_kBest = kBest.fit_transform(X_train_high_variance, y)\n",
    "X_test_kBest = kBest.transform(X_test_high_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)\n",
    "X_train_pca = pca.fit_transform(X_train_kBest)\n",
    "X_test_pca = pca.transform(X_test_kBest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_18 (Dense)            (None, 128)               22400     \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,721\n",
      "Trainable params: 30,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# get the number of input features in X and assign to n_features\n",
    "n_features = X_selected.shape[1]\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer with 150 neurons and specify the input shape\n",
    "model.add(Dense(128, input_dim=n_features, activation='relu'))\n",
    "\n",
    "# Add the second hidden layer with 25 neurons\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Add the output layer with 1 neuron (for binary classification) and 'sigmoid' activation\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# Print the model summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) #change optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1135/1135 [==============================] - 3s 2ms/step - loss: -15739112128512.0000 - accuracy: 0.0000e+00 - val_loss: -56173973733376.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -198470438748160.0000 - accuracy: 0.0000e+00 - val_loss: -365391457026048.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -720313864159232.0000 - accuracy: 0.0000e+00 - val_loss: -1032758038102016.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -1658082190950400.0000 - accuracy: 0.0000e+00 - val_loss: -2108902896304128.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1135/1135 [==============================] - 3s 2ms/step - loss: -3069097998811136.0000 - accuracy: 0.0000e+00 - val_loss: -3650613753675776.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1135/1135 [==============================] - 3s 3ms/step - loss: -5000381314629632.0000 - accuracy: 0.0000e+00 - val_loss: -5690149841141760.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1135/1135 [==============================] - 3s 2ms/step - loss: -7497378126888960.0000 - accuracy: 0.0000e+00 - val_loss: -8277687248355328.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -10623250493079552.0000 - accuracy: 0.0000e+00 - val_loss: -11485393237049344.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1135/1135 [==============================] - 3s 2ms/step - loss: -14441335759044608.0000 - accuracy: 0.0000e+00 - val_loss: -15347330119106560.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1135/1135 [==============================] - 3s 3ms/step - loss: -18989665323319296.0000 - accuracy: 0.0000e+00 - val_loss: -19907160532058112.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1135/1135 [==============================] - 3s 2ms/step - loss: -24335404958220288.0000 - accuracy: 0.0000e+00 - val_loss: -25240049624809472.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -30504796913926144.0000 - accuracy: 0.0000e+00 - val_loss: -31343492357685248.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -37578807766417408.0000 - accuracy: 0.0000e+00 - val_loss: -38320020337459200.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1135/1135 [==============================] - 3s 2ms/step - loss: -45615490441150464.0000 - accuracy: 0.0000e+00 - val_loss: -46225762344239104.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1135/1135 [==============================] - 3s 2ms/step - loss: -54657470341054464.0000 - accuracy: 0.0000e+00 - val_loss: -55066729184755712.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1135/1135 [==============================] - 3s 2ms/step - loss: -64738806556786688.0000 - accuracy: 0.0000e+00 - val_loss: -64877920547504128.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1135/1135 [==============================] - 3s 2ms/step - loss: -75930658106507264.0000 - accuracy: 0.0000e+00 - val_loss: -75773762951184384.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1135/1135 [==============================] - 3s 2ms/step - loss: -88306563420258304.0000 - accuracy: 0.0000e+00 - val_loss: -87770998178316288.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -101917589659189248.0000 - accuracy: 0.0000e+00 - val_loss: -100951110348439552.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -116775754141794304.0000 - accuracy: 0.0000e+00 - val_loss: -115278623031689216.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -132961235317555200.0000 - accuracy: 0.0000e+00 - val_loss: -130881491893747712.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -150521887712083968.0000 - accuracy: 0.0000e+00 - val_loss: -147774517391917056.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -169514181555388416.0000 - accuracy: 0.0000e+00 - val_loss: -166026341693521920.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -189987740899606528.0000 - accuracy: 0.0000e+00 - val_loss: -185663378847432704.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -211997438246912000.0000 - accuracy: 0.0000e+00 - val_loss: -206735914330750976.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -235529151744835584.0000 - accuracy: 0.0000e+00 - val_loss: -229200981290647552.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -260665671182974976.0000 - accuracy: 0.0000e+00 - val_loss: -253247506748538880.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -287513322771709952.0000 - accuracy: 0.0000e+00 - val_loss: -278861386031824896.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -316039224241422336.0000 - accuracy: 0.0000e+00 - val_loss: -306060469024587776.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -346397530558300160.0000 - accuracy: 0.0000e+00 - val_loss: -335018547283492864.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -378657854552276992.0000 - accuracy: 0.0000e+00 - val_loss: -365737940090880000.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -412784359016235008.0000 - accuracy: 0.0000e+00 - val_loss: -398135634318852096.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -448778040382586880.0000 - accuracy: 0.0000e+00 - val_loss: -432263410093129728.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -486650718401331200.0000 - accuracy: 0.0000e+00 - val_loss: -468204624138993664.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -526598690257764352.0000 - accuracy: 0.0000e+00 - val_loss: -506179281861214208.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -568642640514383872.0000 - accuracy: 0.0000e+00 - val_loss: -546028881786699776.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -612962339322331136.0000 - accuracy: 0.0000e+00 - val_loss: -588071388934307840.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -659511607193239552.0000 - accuracy: 0.0000e+00 - val_loss: -632121879192141824.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -708192759390928896.0000 - accuracy: 0.0000e+00 - val_loss: -678230449058742272.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -759210442517118976.0000 - accuracy: 0.0000e+00 - val_loss: -726504575795724288.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -812581492843610112.0000 - accuracy: 0.0000e+00 - val_loss: -776923025084776448.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -868219117671284736.0000 - accuracy: 0.0000e+00 - val_loss: -829475901321248768.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -926425201661444096.0000 - accuracy: 0.0000e+00 - val_loss: -884542948333584384.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -987091030601891840.0000 - accuracy: 0.0000e+00 - val_loss: -941773902948859904.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -1050315423100174336.0000 - accuracy: 0.0000e+00 - val_loss: -1001417254794952704.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -1116275744125747200.0000 - accuracy: 0.0000e+00 - val_loss: -1063715927222124544.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -1184754015498403840.0000 - accuracy: 0.0000e+00 - val_loss: -1128235063381590016.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -1255819300292263936.0000 - accuracy: 0.0000e+00 - val_loss: -1195087363215196160.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1135/1135 [==============================] - 3s 2ms/step - loss: -1329587047228243968.0000 - accuracy: 0.0000e+00 - val_loss: -1264702529610973184.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1135/1135 [==============================] - 2s 2ms/step - loss: -1406388759062118400.0000 - accuracy: 0.0000e+00 - val_loss: -1337109768346533888.0000 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20a9ed96020>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_selected, y, epochs=50, batch_size=128, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test_high_variance, y)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2431/2431 [==============================] - 3s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "md_probs = model.predict(X_testscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(testX, testy, batch_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "md_probs = scaler.transform(X_test)  # Don't fit_transform on the test set, use the scaler fitted on the training set\n",
    "loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Flatten md_probs (assuming it's a 2D array)\n",
    "flat_predictions = md_probs.flatten()\n",
    "\n",
    "# Create a DataFrame with 'row ID' and predicted values\n",
    "predicted_df = pd.DataFrame({\n",
    "    'row ID': additional_data['row ID'],\n",
    "    'price_doc': flat_predictions  # Replace with the actual column name for your predictions\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "predicted_df.to_csv('predictions52.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
